<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="x-ua-compatible" content="ie=edge" />
    <title>CVPR'25 - CoLLM</title>
    <meta name="description" content="CoLLM: A Large Language Model for Composed Image Retrieval. Accepted to CVPR 2025" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="style/main.css">
    <link rel="stylesheet" href="style/glide.core.min.css">
    <link rel="stylesheet" href="style/glide.theme.min.css">
    <link rel="stylesheet" href="style/glide-custom.css">
    <link rel="stylesheet" href="style/bulma.min.css">
    
    <link rel="apple-touch-icon" sizes="180x180" href="favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon/favicon-16x16.png">
    <link rel="manifest" href="favicon/site.webmanifest">
    <link rel="mask-icon" href="favicon/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
  </head>
  <body>
    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
          <a class="navbar-item" href="https://hmchuong.github.io">
            <span class="icon">
              <svg class="svg-inline--fa fa-home fa-w-18" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="home" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" data-fa-i2svg=""><path fill="currentColor" d="M280.37 148.26L96 300.11V464a16 16 0 0 0 16 16l112.06-.29a16 16 0 0 0 15.92-16V368a16 16 0 0 1 16-16h64a16 16 0 0 1 16 16v95.64a16 16 0 0 0 16 16.05L464 480a16 16 0 0 0 16-16V300L295.67 148.26a12.19 12.19 0 0 0-15.3 0zM571.6 251.47L488 182.56V44.05a12 12 0 0 0-12-12h-56a12 12 0 0 0-12 12v72.61L318.47 43a48 48 0 0 0-61 0L4.34 251.47a12 12 0 0 0-1.6 16.9l25.5 31A12 12 0 0 0 45.15 301l235.22-193.74a12.19 12.19 0 0 1 15.3 0L530.9 301a12 12 0 0 0 16.9-1.6l25.5-31a12 12 0 0 0-1.7-16.93z"></path></svg><!-- <i class="fas fa-home"></i> Font Awesome fontawesome.com -->
            </span>
          </a>
  
          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link">
              More Research
            </a>
            <div class="navbar-dropdown">
              <a class="navbar-item" href="https://github.com/vkhoi/cora_cvpr24" target="_blank">
                CORA - CVPR'24
              </a>
              <a class="navbar-item" href="https://maggie-matt.github.io/" target="_blank">
                MaGGIe - CVPR'24
              </a>
              <a class="navbar-item" href="https://simpson-cvpr23.github.io/" target="_blank">
                SimpSON - CVPR'23
              </a>
              <a class="navbar-item" href="https://github.com/VinAIResearch/MagNet" target="_blank">
                MagNet - CVPR'21
              </a>
            </div>
          </div>
        </div>
  
      </div>
    </nav>
    
    <!-- For the title -->
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">CoLLM: A Large Language Model for Composed Image Retrieval</h1>
              <div class="is-size-3" id="conference">CVPR 2025</div>
              <br/>
              <div class="is-size-4 publication-authors">
                <span class="author-block">
                  <a href="https://hmchuong.github.io">Chuong Huynh</a><sup>1,*</sup>
                </span>
                <span class="author-block">
                  <a href="https://viyjy.github.io/">Jinyu Yang</a><sup>2,†</sup>
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=BGjEep8AAAAJ&hl=en">Ashish Tawari</a><sup>2</sup>
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=en&user=p8gsO3gAAAAJ">Mubarak Shah</a><sup>2,3</sup>
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=en&user=P0EbpmgAAAAJ">Son Tran</a><sup>2</sup>
                </span>
              </div>
              <div class="is-size-4 publication-authors">
                <span class="author-block">
                  <a href="https://www.raffayhamid.com/">Raffay Hamid</a><sup>2</sup>
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=DrNeo_0AAAAJ&hl=en">Trishul Chilimbi</a><sup>2</sup>
                </span>
                <span class="author-block">
                  <a href="http://www.cs.umd.edu/~abhinav">Abhinav Shrivastava</a><sup>1</sup>
                </span>
              </div>
              
              <br/>
              <div class="is-size-4 publication-authors">
                <span class="author-block company-block"><sup>1</sup>University of Maryland, College Park</span>
                <span class="author-block company-block" style="flex:0.6"><sup>2</sup>Amazon</span>
              </div>
              <div class="is-size-4 publication-authors">
                <span class="author-block company-block"><sup>3</sup>Center for Research in Computer Vision, University of Central Florida</span>
              </div>
              <div class="is-size-5">
                <span><em>( <sup>*</sup>This work was done during internship at Amazon. <sup>†</sup>Project Lead)</em></span>
              </div>
              <br/>
              <div class="column has-text-centered">
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark paper-link">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv (soon)</span>
                  </a>
                </span>
                <span class="link-block">
                </span>
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
                    </span>
                    <span>Code (soon)</span>
                  </a>
                </span>
                
                <span class="link-block">
                <a href=""
                    class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <img src="dist/database.svg"/>
                  </span>
                  <span>Dataset (soon)</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    
    <!-- For the teaser video -->
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <img id="overall" src="images/cir.png" style="width: 80%"/>
          <h2 class="subtitle has-text-centered">
            <span class="maggie">CoLLM</span>: An LLM for retrieving the most relevant image by composing a reference image with textual modifications.
          </h2>
        </div>
      </div>
    </section>

    <!-- For the abstract/ method -->
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h2 class="title is-3">Abstract</h2>
              <div class="content has-text-justified">
                <p>
                  Composed Image Retrieval (CIR) is a complex task that aims to retrieve images based on a multimodal query. Typical training data consists of triplets containing a reference image, a textual description of desired modifications, and the target image, which are expensive and time-consuming to acquire. The scarcity of CIR datasets has led to zero-shot approaches utilizing synthetic triplets or leveraging vision-language models (VLMs) with ubiquitous web-crawled image-caption pairs. However, these methods have significant limitations: synthetic triplets suffer from limited scale, lack of diversity, and unnatural modification text, while image-caption pairs hinder joint embedding learning of the multimodal query due to the absence of triplet data Moreover, existing approaches struggle with complex and nuanced modification texts that demand sophisticated fusion and understanding of vision and language modalities. We present <span class="maggie">CoLLM</span>, a one-stop framework that effectively addresses these limitations. <span class="highlight">Our approach generates triplets on-the-fly from image-caption pairs, enabling supervised training without manual annotation.</span> We leverage Large Language Models (LLMs) to generate joint embeddings of reference images and modification texts, facilitating deeper multimodal fusion. Additionally, <span class="highlight">we introduce <span class="maggie">Multi-Text CIR (MTCIR)</span>, a large-scale dataset</span> comprising 3.4M samples, and <span class="highlight"><span class="maggie">refine existing CIR benchmarks</span> (CIRR and Fashion-IQ) to enhance evaluation reliability</span>. Experimental results demonstrate that CoLLM achieves state-of-the-art performance across multiple CIR benchmarks and settings. MTCIR yields competitive results, with up to 15% performance improvement. Our refined benchmarks provide more reliable evaluation metrics for CIR models, contributing to the advancement of this important field.
              </div>
            </div>
          </div>
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h2 class="title is-3"><span class="maggie">CoLLM</span> framework</h2>
              <div class="content has-text-justified">
                <p>
                  Our model is flexibly trained with <span class="highlight"> (a) image-caption pairs</span> and <span class="highlight"> (b) Composed image retrieval triplets</span>. Image and text in the training query are synthesized by interpolated between similar in-batch image-caption pairs.
                </p>
              </div>
              <div class="columns is-centered">
                <div class="column framework">
                  <img id="overall" src="images/training.png" style="width: 100%; object-fit: contain;"/>
                </div>
                <div class="column framework">
                  <img id="synthesis" src="images/synthesis.png" style="width: 100%; object-fit: contain;"/>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    
    <!-- For datasets-->
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column ">
              <h2 class="title is-3 has-text-centered"><span class="maggie">MTCIR:</span> Multi-Text Composed Image Retrieval Dataset</h2>
              <div class="content has-text-justified">
                <p>
                  We create <span class="highlight">the largest public dataset</span> with diverse samples: 3.4M triplets and 17.7M human-aligned modification texts generated by <a href="https://claude.ai/" target="_blank">Claude 3 Sonnet</a>.
                </p>
                <img class="figure" src="images/mtcir.png" style="width: 100%;"/>
              </div>
            </div>
          </div>
          <div class="columns is-centered">
            <div class="column ">
              <h2 class="title is-3 has-text-centered"><span class="maggie">Refined Benchmarks:</span> CIRR and Fashion-IQ</h2>
              <div class="content has-text-justified">
                <p>
                  We enhance refined benchmarks by rewrite modification texts of ambiguous samples with <a href="https://claude.ai/" target="_blank">Claude 3 Sonnet</a>. The comprehensive pipeline is shown on the left, and an refined example is shown on the right:
                </p>
                <div class="columns is-centered">
                  <div class="column framework">
                    <img id="overall" src="images/benchmark_pipeline.png"/>
                  </div>
                  <div class="column framework">
                    <img id="synthesis" src="images/benchmark_examples.png"/>
                  </div>
                </div>
                <p>
                  Examples of "good" samples kept in the refined benchmarks: CIRR (left) and Fashion-IQ (right)
                </p>
                <img class="figure" src="images/refined_good.png" style="width: 100%;"/>
                <p>
                  Examples of "bad" samples and regenerated texts in the refined benchmarks: CIRR (left) and Fashion-IQ (right)
                </p>
                <img class="figure" src="images/refined_bad.png" style="width: 100%;"/>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- For datasets-->
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column ">
              <h2 class="title is-3 has-text-centered"><span class="maggie">CoLLM</span> outperforms previous methods</h2>
              <div class="content has-text-justified">
                <p>
                  Our framework achieves the highest Recall Sum at {1,10,50} for CIRR and {10, 50} for Fashion-IQ with two training scenarios: (i) without triplet data and (ii) with synthetic triplet data (our MTCIR).
                </p>
                <div class="columns is-centered">
                  <img  class="figure" src="images/quantitative.png" style="width: 75%;"/>
                </div>
              </div>
            </div>
          </div>
          <div class="columns is-centered">
            <div class="column ">
              <h2 class="title is-3 has-text-centered"><span class="maggie">CoLLM</span> trained on MTCIR has better CIR capacity.</h2>
              <div class="content has-text-justified">
                <p>
                  Our model shows better composed query understanding in both CIRR and Fashion-IQ benchmarks, even without training on triplets:
                </p>
              </div>
              <div class="container" id="glide-container" style="position: relative;">
                <div class="glide" id="qual_results1">
                  <div data-glide-el="track" class="glide__track">
                    <ul class="glide__slides">
                      <li class="glide__slide">
                        <img src="images/qual1.png">
                      </li>
                      <li class="glide__slide">
                        <img src="images/qual2.png">
                      </li>
                      <li class="glide__slide">
                        <img src="images/qual3.png">
                      </li>
                      <li class="glide__slide">
                        <img src="images/qual4.png">
                      </li>
                    </ul>
                  </div>
                  <div class="glide__arrows" data-glide-el="controls" style="position: absolute; width: 100%; top: 50%; transform: translateY(-50%); pointer-events: none;">
                    <span class="glide__arrow glide__arrow--left" data-glide-dir="<" style="position: absolute; left: -4rem; pointer-events: auto;"><img src="dist/arrow-left-circle-fill.svg"/></span>
                    <span class="glide__arrow glide__arrow--right" data-glide-dir=">" style="position: absolute; right: -4rem; pointer-events: auto;"><img src="dist/arrow-right-circle-fill.svg"/></span>
                  </div>
                </div>
              </div>
              <div class="content has-text-justified">
                <p>
                  The new refined text helps resolving the ambiguity in the benchmarks, the model can now retrieve the correct target image:
                </p>
              </div>
              <div class="container" id="glide-container" style="position: relative;">
                <div class="glide" id="qual_results2">
                  <div data-glide-el="track" class="glide__track">
                    <ul class="glide__slides">
                      <li class="glide__slide">
                        <img src="images/ref_qual1.png">
                      </li>
                      <li class="glide__slide">
                        <img src="images/ref_qual2.png">
                      </li>
                      <li class="glide__slide">
                        <img src="images/ref_qual3.png">
                      </li>
                      <li class="glide__slide">
                        <img src="images/ref_qual4.png">
                      </li>
                    </ul>
                  </div>
                  <div class="glide__arrows" data-glide-el="controls" style="position: absolute; width: 100%; top: 50%; transform: translateY(-50%); pointer-events: none;">
                    <span class="glide__arrow glide__arrow--left" data-glide-dir="<" style="position: absolute; left: -4rem; pointer-events: auto;"><img src="dist/arrow-left-circle-fill.svg"/></span>
                    <span class="glide__arrow glide__arrow--right" data-glide-dir=">" style="position: absolute; right: -4rem; pointer-events: auto;"><img src="dist/arrow-right-circle-fill.svg"/></span>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@InProceedings{huynh2025collm,
    author    = {Huynh, Chuong and Yang, Jinyu and Tawari, Ashish and Shah, Mubarak and Tran, Son and Hamid, Raffay and Chilimbi, Trishul and Shrivastava, Abhinav},
    title     = {CoLLM: A Large Language Model for Composed Image Retrieval},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year      = {2025}
}</code>
        </pre>
      </div>
    </section>
    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This website borrows the source code from <a href="https://nerfies.github.io/">nerfies</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

    </div>
    
    <script
      src="https://code.jquery.com/jquery-3.2.1.js"
      integrity="sha256-DZAnKJ/6XZ9si04Hgrsxu/8s717jcIzLy3oi35EouyE="
      crossorigin="anonymous"></script>
    <script src="js/main.js"></script>
    <script src="dist/cocoen.js"></script>
    <script src="dist/glide.min.js"></script>
    <script>
      Cocoen.parse(document.body);
    </script>
    <script>
      glide = new Glide('#qual_results1', {
        type: "carousel",
        perView: 1.0,
        focusAt: "center",
        autoplay: 5000,
        hoverpause: true
      }).mount();
      glide = new Glide('#qual_results2', {
        type: "carousel",
        perView: 1.0,
        focusAt: "center",
        autoplay: 5000,
        hoverpause: true
      }).mount();
  </script>
  </body>
</html>
